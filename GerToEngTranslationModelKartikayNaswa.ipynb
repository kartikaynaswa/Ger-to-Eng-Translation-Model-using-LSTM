{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import load,dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def Clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the data \"deu.txt\" is downloaded and saved in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "file = open('deu.txt', mode='rt', encoding='utf-8')\n",
    "doc = file.read()\n",
    "file.close()\n",
    "# split into english-german pairs\n",
    "pairs = make_pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = Clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'english-german.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing And Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    a = load(open(filename, 'rb'))\n",
    "    return a[:,:2]\n",
    "    # a is sliced to remove extra info. in the data and keep only english and german sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-german.pkl')\n",
    "# reduce dataset size\n",
    "n_sentences = 20000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:18000], dataset[18000:]\n",
    "# save\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "save_clean_data(test, 'english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initializing And Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 3515\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 5565\n",
      "German Max Length: 10\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 256)           1424640   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               525312    \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 5, 256)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 5, 256)            525312    \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 5, 3515)          903355    \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,378,619\n",
      "Trainable params: 3,378,619\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 3.57680, saving model to model.h5\n",
      "282/282 - 29s - loss: 4.1772 - val_loss: 3.5768 - 29s/epoch - 103ms/step\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 3.57680 to 3.34117, saving model to model.h5\n",
      "282/282 - 22s - loss: 3.4643 - val_loss: 3.3412 - 22s/epoch - 77ms/step\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 3.34117 to 3.12710, saving model to model.h5\n",
      "282/282 - 21s - loss: 3.1954 - val_loss: 3.1271 - 21s/epoch - 73ms/step\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 3.12710 to 2.91887, saving model to model.h5\n",
      "282/282 - 20s - loss: 2.9433 - val_loss: 2.9189 - 20s/epoch - 71ms/step\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 2.91887 to 2.70388, saving model to model.h5\n",
      "282/282 - 20s - loss: 2.6932 - val_loss: 2.7039 - 20s/epoch - 72ms/step\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 2.70388 to 2.51109, saving model to model.h5\n",
      "282/282 - 20s - loss: 2.4338 - val_loss: 2.5111 - 20s/epoch - 72ms/step\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 2.51109 to 2.35755, saving model to model.h5\n",
      "282/282 - 20s - loss: 2.2042 - val_loss: 2.3575 - 20s/epoch - 72ms/step\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 2.35755 to 2.25024, saving model to model.h5\n",
      "282/282 - 21s - loss: 2.0095 - val_loss: 2.2502 - 21s/epoch - 74ms/step\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 2.25024 to 2.16869, saving model to model.h5\n",
      "282/282 - 21s - loss: 1.8409 - val_loss: 2.1687 - 21s/epoch - 74ms/step\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 2.16869 to 2.08234, saving model to model.h5\n",
      "282/282 - 21s - loss: 1.6869 - val_loss: 2.0823 - 21s/epoch - 74ms/step\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 2.08234 to 2.00971, saving model to model.h5\n",
      "282/282 - 21s - loss: 1.5416 - val_loss: 2.0097 - 21s/epoch - 74ms/step\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 2.00971 to 1.96040, saving model to model.h5\n",
      "282/282 - 21s - loss: 1.4063 - val_loss: 1.9604 - 21s/epoch - 75ms/step\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 1.96040 to 1.90014, saving model to model.h5\n",
      "282/282 - 21s - loss: 1.2803 - val_loss: 1.9001 - 21s/epoch - 75ms/step\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 1.90014 to 1.84890, saving model to model.h5\n",
      "282/282 - 23s - loss: 1.1607 - val_loss: 1.8489 - 23s/epoch - 82ms/step\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 1.84890 to 1.80551, saving model to model.h5\n",
      "282/282 - 24s - loss: 1.0487 - val_loss: 1.8055 - 24s/epoch - 84ms/step\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 1.80551 to 1.77025, saving model to model.h5\n",
      "282/282 - 23s - loss: 0.9433 - val_loss: 1.7703 - 23s/epoch - 80ms/step\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 1.77025 to 1.74370, saving model to model.h5\n",
      "282/282 - 21s - loss: 0.8498 - val_loss: 1.7437 - 21s/epoch - 74ms/step\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 1.74370 to 1.71467, saving model to model.h5\n",
      "282/282 - 21s - loss: 0.7635 - val_loss: 1.7147 - 21s/epoch - 73ms/step\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 1.71467 to 1.70320, saving model to model.h5\n",
      "282/282 - 21s - loss: 0.6855 - val_loss: 1.7032 - 21s/epoch - 74ms/step\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 1.70320 to 1.68883, saving model to model.h5\n",
      "282/282 - 21s - loss: 0.6168 - val_loss: 1.6888 - 21s/epoch - 74ms/step\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.68883\n",
      "282/282 - 21s - loss: 0.5562 - val_loss: 1.6927 - 21s/epoch - 74ms/step\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 1.68883 to 1.67462, saving model to model.h5\n",
      "282/282 - 21s - loss: 0.5028 - val_loss: 1.6746 - 21s/epoch - 76ms/step\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 1.67462 to 1.67462, saving model to model.h5\n",
      "282/282 - 21s - loss: 0.4539 - val_loss: 1.6746 - 21s/epoch - 75ms/step\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 1.67462 to 1.66971, saving model to model.h5\n",
      "282/282 - 21s - loss: 0.4135 - val_loss: 1.6697 - 21s/epoch - 75ms/step\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.66971\n",
      "282/282 - 21s - loss: 0.3768 - val_loss: 1.6769 - 21s/epoch - 75ms/step\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.66971\n",
      "282/282 - 21s - loss: 0.3440 - val_loss: 1.6865 - 21s/epoch - 76ms/step\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.66971\n",
      "282/282 - 21s - loss: 0.3147 - val_loss: 1.6929 - 21s/epoch - 75ms/step\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.66971\n",
      "282/282 - 22s - loss: 0.2909 - val_loss: 1.6947 - 22s/epoch - 77ms/step\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.66971\n",
      "282/282 - 21s - loss: 0.2695 - val_loss: 1.7000 - 21s/epoch - 75ms/step\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.66971\n",
      "282/282 - 21s - loss: 0.2497 - val_loss: 1.6995 - 21s/epoch - 75ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25951af7940>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "\n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "# fit model\n",
    "filename = 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted, a, p = list(), list(), list(), list()\n",
    "    \n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        # code to print translations\n",
    "        # if i < 10:\n",
    "        #     print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        # else :\n",
    "        #     break\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "        a.append(raw_target)\n",
    "        p.append(translation)\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    \n",
    "    return pd.DataFrame({'Actual': a,'Predicted' : p})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train BLEU score\n",
      "BLEU-1: 0.852941\n",
      "BLEU-2: 0.777282\n",
      "BLEU-3: 0.737552\n",
      "BLEU-4: 0.574258\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i detest tom</td>\n",
       "      <td>i hate tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw the doctor</td>\n",
       "      <td>i saw the doctor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i got ripped off</td>\n",
       "      <td>i got ripped off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i live here</td>\n",
       "      <td>im here here home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>get your mother</td>\n",
       "      <td>get your mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he deserved it</td>\n",
       "      <td>he deserved it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i have a cough</td>\n",
       "      <td>i have a cough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stop squabbling</td>\n",
       "      <td>stop squabbling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>he called me fat</td>\n",
       "      <td>he made me fat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>did i win</td>\n",
       "      <td>did i win</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Actual          Predicted\n",
       "0      i detest tom         i hate tom\n",
       "1  i saw the doctor   i saw the doctor\n",
       "2  i got ripped off   i got ripped off\n",
       "3       i live here  im here here home\n",
       "4   get your mother    get your mother\n",
       "5    he deserved it     he deserved it\n",
       "6    i have a cough     i have a cough\n",
       "7   stop squabbling    stop squabbling\n",
       "8  he called me fat     he made me fat\n",
       "9         did i win          did i win"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('train BLEU score')\n",
    "train_translation = evaluate_model(model, eng_tokenizer, trainX[:10], train[:10])\n",
    "train_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test BLEU score\n",
      "BLEU-1: 0.652174\n",
      "BLEU-2: 0.542706\n",
      "BLEU-3: 0.450572\n",
      "BLEU-4: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kartikay Naswa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who are you</td>\n",
       "      <td>who are you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please smile</td>\n",
       "      <td>say cheese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is tom tall</td>\n",
       "      <td>is tom tall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i hear voices</td>\n",
       "      <td>i think so now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ill fix that</td>\n",
       "      <td>ill fix that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>youre vain</td>\n",
       "      <td>youre vain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i never bet</td>\n",
       "      <td>i never hope it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i tidied my room</td>\n",
       "      <td>i washed my house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>im so cold</td>\n",
       "      <td>i feel so cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>can you open it</td>\n",
       "      <td>can you open this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>he is skating</td>\n",
       "      <td>he surrendered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>good for you</td>\n",
       "      <td>good for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>im a christian</td>\n",
       "      <td>im am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>it couldnt hurt</td>\n",
       "      <td>it cant hurt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>may i come in</td>\n",
       "      <td>can i come in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Actual          Predicted\n",
       "0        who are you        who are you\n",
       "1       please smile         say cheese\n",
       "2        is tom tall        is tom tall\n",
       "3      i hear voices     i think so now\n",
       "4       ill fix that       ill fix that\n",
       "5         youre vain         youre vain\n",
       "6        i never bet    i never hope it\n",
       "7   i tidied my room  i washed my house\n",
       "8         im so cold     i feel so cold\n",
       "9    can you open it  can you open this\n",
       "10     he is skating     he surrendered\n",
       "11      good for you           good for\n",
       "12    im a christian              im am\n",
       "13   it couldnt hurt       it cant hurt\n",
       "14     may i come in      can i come in"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on some test sequences\n",
    "print('test BLEU score')\n",
    "test_translation = evaluate_model(model, eng_tokenizer, testX[95:110], test[95:110])\n",
    "test_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5421b1e2dab826023ebe3dac2abf947bae1182acb6e354c0f14dce91658111bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
